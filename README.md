# celery_zhilian
celery分布式任务构建智联招聘全站采集
# 项目说明：
- 第一次上传github项目，智联招聘近千万数据量，scarpy框架抓取，效率较慢，扩展性太差。才用celery异步任务来分步抓取全站，redis用作消息中间件，任务存储结果，url分发，基于布隆过滤器集成增量式分布爬虫。
1.需要代理ip,可以使用第三方代理或者自建的代理ip池，对于未请求到数据的url会进行更换代理重试，尽量保证数据完全
2.定义三个异步任务，分别是，区域url抓取、列表url抓取、详情url页面（存储）抓取，
3.redis最好使用管道操作，由于单线程，效率低，开启持久化机制（rdb，aof），mysql批量存储数据
4.item存储为完成，后期有时间再上传

